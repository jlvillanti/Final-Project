# -*- coding: utf-8 -*-
"""4/25 - Final Project - Hannah Bottomley.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KV1ZsSPjI4jhDkzZOo1WbzJ-gMLOceLr
"""



"""# Data Loading
**Reasoning**: You need to load the data into a structured format (Pandas DataFrame) to perform cleaning, filtering, and modeling.
"""

# prompt: Load the dataset kaggle_survey_2022_responses from my google drive using this pathway "/content/drive/MyDrive/Colab Notebooks/kaggle_survey_2022_responses.csv" into a pandas DataFrame.

import pandas as pd

# Load the dataset from Google Drive
try:
    df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/kaggle_survey_2022_responses.csv')
    display(df.head())
    print(df.shape)
except FileNotFoundError:
    print("Error: '/content/drive/MyDrive/Colab Notebooks/kaggle_survey_2022_responses.csv' not found.")
    df = None
except pd.errors.ParserError:
    print("Error: Could not parse the CSV file.")
    df = None
except Exception as e:
    print(f"An unexpected error occurred: {e}")
    df = None

from google.colab import drive
drive.mount('/content/drive')



"""# Data Exploration

Explore the dataset to understand its structure, identify potential issues like missing values and data types, and get a sense of the distribution of variables, especially those related to salary (column 'Q24').
"""

import matplotlib.pyplot as plt
import seaborn as sns

# 1. Examine Data Structure
print(df.shape)
display(df.head())
display(df.info())

# 2. Missing Values
missing_values = df.isnull().sum()
print("Missing values in each column:\n", missing_values)
print("\nMissing values in 'Q24':", df['Q24'].isnull().sum())


# 3. Data Distribution of 'Q24'
print("\nDescriptive statistics for 'Q24':\n", df['Q24'].describe())
plt.figure(figsize=(10, 6))
plt.hist(df['Q24'].dropna(), bins=30)
plt.title('Distribution of Yearly Compensation')
plt.xlabel('Yearly Compensation')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x=df['Q24'].dropna())
plt.title('Box Plot of Yearly Compensation')
plt.show()


# 4. Correlation Analysis (Optional) - Focusing on numerical columns
# For this step, we need to convert relevant columns to numeric.
# However, this step might not be straightforward since salary data is categorized.
# We will explore a basic correlation if any column is numeric.
numeric_cols = df.select_dtypes(include=['number'])

if not numeric_cols.empty:
    correlation = numeric_cols.corr()['Q24'].drop('Q24')
    print("Correlation with Q24\n", correlation)


# 5. Categorical Variable Exploration - focusing on a few key columns
categorical_cols = ['Q5', 'Q22']  # Examples. Replace with actual relevant cols
for col in categorical_cols:
    print(f"\nValue counts for '{col}':\n{df[col].value_counts()}")
    plt.figure(figsize=(10, 6))
    df[col].value_counts().plot(kind='bar')
    plt.title(f"Distribution of '{col}'")
    plt.show()

"""# Data Cleaning

Clean the data by handling missing values, addressing inconsistencies in data types, removing duplicates, and managing outliers in the 'Q24' column. Reasoning: Clean the data by handling missing values, addressing inconsistencies in data types, removing duplicates, and managing outliers in the 'Q24' column. This involves several steps, including data type conversion, outlier removal using IQR, and missing value imputation.
"""

import numpy as np

# Convert 'Q24' to numeric, handling non-numeric characters
df_cleaned = df.copy()
df_cleaned['Q24'] = df_cleaned['Q24'].astype(str).str.replace(r'[$,]', '', regex=True)
df_cleaned['Q24'] = pd.to_numeric(df_cleaned['Q24'], errors='coerce')

print(f"Number of unique values in Q24 after numeric conversion: {df_cleaned['Q24'].nunique()}")
print(f"Data type of Q24 after conversion: {df_cleaned['Q24'].dtype}")
print(df_cleaned['Q24'].describe())

# Outlier handling: Cap at 99th percentile
upper_limit = df_cleaned['Q24'].quantile(0.99)
df_cleaned['Q24'] = np.where(df_cleaned['Q24'] > upper_limit, upper_limit, df_cleaned['Q24'])

# Fill missing values with the median
median_q24 = df_cleaned['Q24'].median()
df_cleaned['Q24'] = df_cleaned['Q24'].fillna(median_q24)

# Remove duplicate rows
df_cleaned.drop_duplicates(inplace=True)

display(df_cleaned.head())
print(df_cleaned.shape)

"""#**Questions**

The following questions that will be kept from the dataset are:
1. What is your age?
2. What is your gender?
3. In which country do you currently reside?
4. On which platforms have you begun or completed data science courses?
5. What products or platforms did you find to be most helpful when you first started studying data science?
6. What is the highest level of formal education that you have attained or plan to attain within the next 2 years?
7. For how many years have you been writing code and/or programming?
8. What programming languages do you use on a regular basis?
9.  Which of the following machine learning frameworks do you use on a regular basis?
10.  Which of the following ML algorithms do you use on a regular basis?
11. Salary
12. How much money have you spent on ML/Cloud services in the last 5 years?
13.  Who/what are your favorite media sources that report on data science topics?

**Reasoning**:
All of these quesetions are the only relevant questions in the dataset that will lead our team to answering all 8 task questions.

In terms of the Gradio app, after testing our data using a random forest to see what variables are truly most relevant in prediciting salary, we will only use include those in our Gradio app site. Regardless of if these selected questions will make it into the Gradio app, there are all esstenial in understanding the trends of becoming a data scientist and understanding a deeper knowledge that goes beyond prediciting a salary.

**Subtopic**

**Combining Multi-select questions and renaming columns**
**Reasoning:**
Combining multi-select questions like "On which platforms have you begun or completed data science courses?" into a single column simplifies the data structure, making it easier to analyze, visualize, and model. It also prepares the data for machine learning by allowing efficient encoding of selections and reducing unnecessary complexity from multiple sparse columns. Also, renaming the column makes it easier to understand what the data is telling us as a team by providing clear, descriptive labels that reduce confusion, improve collaboration, and ensure everyone—analysts, developers, and stakeholders alike—can quickly interpret the meaning and purpose of each column without needing to refer back to the original survey or codebook.

**Subtopic**:**Select Revelant Columns**

Keeps only the columns relevant to:

*   Predicting salary
*   Analyzing tool/tech use
*   Understanding education trends This avoids memory issues and speeds up processing.

**Reasoning**:
1. Improves Performance mance (Speed + Memory)
  - espcially important for notebooks like Colab, that are limitd on memory
2. Focuses Analysis on Your Project Goals
  - Predict salary
  - Understand tool usage trends
  - Explore educuation pathways
3. Prevents Overfitting and Noise in Model
  - helps prevent learning models to "learn" from unimportant variables
4. Makes Data easier to Work with

**Full Cleaning + Encoding Code (to get ready for modeling)**
"""

import pandas as pd

# ✅ Load your dataset
file_path = "/content/drive/MyDrive/Colab Notebooks/kaggle_survey_2022_responses.csv"
df = pd.read_csv(file_path, header=1, low_memory=False)

# ✅ Define relevant columns to keep
keywords_to_keep = [
    "What is your age", "What is your gender", "In which country do you currently reside",
    "On which platforms have you begun or completed data science courses",
    "What products or platforms did you find to be most helpful when you first started",
    "What is the highest level of formal education", "For how many years have you been writing code",
    "What programming languages do you use on a regular basis",
    "Which of the following machine learning frameworks do you use",
    "Which of the following ML algorithms do you use",
    "What is your current yearly compensation",
    "Approximately how much money have you spent on machine learning",
    "Who/what are your favorite media sources that report on data science topics"
]

# ✅ Keep only relevant columns
df_filtered = df[[col for col in df.columns if any(keyword in col for keyword in keywords_to_keep)]].copy()

# ✅ Rename important columns
df_filtered.rename(columns={
    "What is your age (# years)?": "What is your age",
    "What is your gender? - Selected Choice": "What is your gender",
    "What is your current yearly compensation (approximate $USD)?": "Salary",
    "Approximately how much money have you spent on machine learning and/or cloud computing services at home or at work in the past 5 years (approximate $USD)?\n (approximate $USD)?":
        "Approximately how much money have you spent on machine learning"
}, inplace=True)

# ✅ Fix text encoding issues (remove 'Äô' and similar artifacts)
def clean_text(x):
    if isinstance(x, str):
        return x.replace("Äô", "'").replace("â€™", "'").replace("â€“", "-").strip()
    return x
df_filtered = df_filtered.applymap(clean_text)

# ✅ Remove $ signs in Salary column
df_filtered["Salary"] = df_filtered["Salary"].replace('[\$,]', '', regex=True)

# ✅ Drop rows with missing Salary
df_filtered = df_filtered.dropna(subset=["Salary"])

# ✅ Combine multi-select columns
def combine_multiselect_columns(df, prefix, new_col_name):
    cols = [col for col in df.columns if prefix in col]
    df[new_col_name] = df[cols].apply(
        lambda row: [val for val in row if pd.notna(val)] or ["No Selection"], axis=1
    )
    df.drop(columns=cols, inplace=True)

combine_multiselect_columns(df_filtered, "On which platforms have you begun or completed data science courses", "On which platforms have you begun or completed data science courses")
combine_multiselect_columns(df_filtered, "What products or platforms did you find to be most helpful when you first started", "What products or platforms did you find to be most helpful when you first started")
combine_multiselect_columns(df_filtered, "What programming languages do you use on a regular basis", "What programming languages do you use on a regular basis")
combine_multiselect_columns(df_filtered, "Which of the following machine learning frameworks do you use", "Which of the following machine learning frameworks do you use")
combine_multiselect_columns(df_filtered, "Which of the following ML algorithms do you use", "Which of the following ML algorithms do you use")
combine_multiselect_columns(df_filtered, "Who/what are your favorite media sources that report on data science topics", "Who/what are your favorite media sources that report on data science topics")

# ✅ Map and clean Education levels
education_mapping = {
    "No formal education past high school": "No formal education past high school",
    "Some college/university study without earning a bachelor’s degree": "Some college/university study without earning a bachelor's degree",
    "Some college/university study without earning a bachelor's degree": "Some college/university study without earning a bachelor's degree",
    "Bachelor’s degree": "Bachelor's degree",
    "Bachelor's degree": "Bachelor's degree",
    "Master’s degree": "Master's degree",
    "Master's degree": "Master's degree",
    "Doctoral degree": "Doctoral degree",
    "Professional doctorate": "Professional doctorate",
    "I prefer not to answer": "Other",
    "Other": "Other"
}

# Apply mapping to the column
df_filtered["What is the highest level of formal education"] = df_filtered[
    "What is the highest level of formal education that you have attained or plan to attain within the next 2 years?"
].map(education_mapping)

# ✅ Make Education an **ordinal** variable
education_order = [
    "No formal education past high school",
    "Some college/university study without earning a bachelor's degree",
    "Bachelor's degree",
    "Master's degree",
    "Doctoral degree",
    "Professional doctorate",
    "Other"
]

df_filtered["What is the highest level of formal education"] = pd.Categorical(
    df_filtered["What is the highest level of formal education"],
    categories=education_order,
    ordered=True
)

# ✅ Clean Gender
df_filtered["What is your gender"] = df_filtered["What is your gender"].replace({
    "Prefer not to say": "Other/Not specified",
    "Prefer to self-describe": "Other/Not specified"
})

# ✅ Group countries (Top 5, rest as "Other")
# FIX: Use "In which country do you currently reside?" which is the actual column name in the DataFrame.
top_5_countries = df_filtered["In which country do you currently reside?"].value_counts().nlargest(5).index.tolist()
df_filtered["What country do you currently reside in?"] = df_filtered["In which country do you currently reside?"].apply(
    lambda x: x if x in top_5_countries else "Other"
)

# ✅ Fix "Who/what are your favorite media sources..." multiple selections
def clean_media_sources(row):
    cleaned = []
    valid_options = [
        "Twitter (data science influencers)",
        "Email newsletters (Data Elixir, O'Reilly Data & AI, etc)",
        "Reddit (r/machinelearning, etc)",
        "Kaggle (notebooks, forums, etc)",
        "Course Forums (forums.fast.ai, Coursera forums, etc)",
        "YouTube (Kaggle YouTube, Cloud AI Adventures, etc)",
        "Podcasts (Chai Time Data Science, O’Reilly Data Show, etc)",
        "Podcasts (Chai Time Data Science, O'Reilly Data Show, etc)", # both curly and straight apostrophes
        "Blogs (Towards Data Science, Analytics Vidhya, etc)",
        "Journal Publications (peer-reviewed journals, conference proceedings, etc)",
        "Slack Communities (ods.ai, kagglenoobs, etc)",
        "None",
        "Other"
    ]
    for source in row:
        if source in valid_options:
            cleaned.append(source)
    if not cleaned:
        cleaned = ["Other"]  # if nothing matched, label it as Other
    return cleaned

df_filtered["Who/what are your favorite media sources that report on data science topics"] = df_filtered[
    "Who/what are your favorite media sources that report on data science topics"
].apply(clean_media_sources)


# ✅ Final columns order
final_columns = [
    "What is your age",
    "What is your gender",
    "What country do you currently reside in?",
    "On which platforms have you begun or completed data science courses",
    "What products or platforms did you find to be most helpful when you first started",
    "What is the highest level of formal education",
    "For how many years have you been writing code and/or programming?",
    "What programming languages do you use on a regular basis",
    "Which of the following machine learning frameworks do you use",
    "Which of the following ML algorithms do you use",
    "Salary",
    "Approximately how much money have you spent on machine learning",
    "Who/what are your favorite media sources that report on data science topics"
]

df_filtered = df_filtered[final_columns]

# ✅ Save the cleaned file with original name
output_path = "/content/kaggle_survey_2022_cleaned_final.csv"
df_filtered.to_csv(output_path, index=False)

# ✅ Rename the file
new_output_path = "/content/kaggle_survey_2022_cleaned_finalproject.csv"

import os
os.rename(output_path, new_output_path)

# ✅ Download the newly renamed file if running in Colab
from google.colab import files
files.download(new_output_path)

"""# Exploring Data Analysis (EDA)

**Understand the Data (Data Analysis First)**

Before we build any app, we need to:

✅ Understand what the data is telling us: Who are the survey respondents? What platforms do they use? How are salary levels distributed?

✅ Identify important patterns: Which features (like education, platforms, coding experience) are correlated with salary?

✅ Clean and structure the data: Real-world data is messy. We need to handle missing values, combine multi-select answers, and convert text into model-ready formats.

👉 Without analysis, we’re flying blind. We wouldn’t even know what kind of model or interface we need.

**Build the Predictive Engine (Random Forest Model)**

Once the data is clean and understood, we build a Random Forest model because:

🌳 It’s a powerful and flexible machine learning algorithm that can handle both numerical and categorical features.

🔍 It helps us understand feature importance (e.g., which inputs most affect salary).

🎯 It serves as the prediction engine behind the app: the app alone can’t predict salaries—the model does.

**Deploy with Confidence (Gradio App)**

Gradio is just the interface. It lets users interact with the model, but:

If the data isn’t prepared properly ➡️ the model will perform poorly.

If the model isn’t trained well ➡️ the app will give inaccurate or misleading results.

By doing analysis + modeling first, we ensure the app is:

✅ Accurate

✅ Insightful

✅ User-friendly

✅ Trustworthy
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load CSV from Drive
import pandas as pd
file_path = "/content/drive/MyDrive/BUS 458 /Copy of kaggle_survey_2022_cleaned-5.csv"
df = pd.read_csv(file_path)
df.head()

"""#**Tasks**

1. What are the most significant factors driving Data Scientist salaries?

**Answer:**To understand what drives compensation in the data science field, we analyzed responses from the 2022 Kaggle Survey using a regression model. The model examined how demographic characteristics, educational background, coding experience, tools used, and learning platforms contribute to predicting salary. The results below represent the top 15 most important features influencing data scientist salaries, based on their feature importance scores in our model:

Top Drivers of Salary:

1. Country (0.2801):
 Geographic location is the strongest predictor of salary. This is due to wide variations in cost of living, demand for data talent, and industry maturity across countries. Data scientists in regions like North America and Western Europe generally report higher salaries.

2. Years of Coding Experience (0.0577):
 More years of coding is directly correlated with higher salaries, as it often translates to deeper technical skills and more advanced job roles.

3. Spending on ML/Cloud Services (0.0543):
 Professionals who invest more in cloud and machine learning tools may be more engaged with modern platforms or projects, which can increase their market value.

4. Age (0.0531):
 Age often reflects a combination of experience, maturity, and seniority, all of which are associated with higher compensation.

5. Education Level (0.0303):
 Formal education still matters. Advanced degrees (e.g., Master’s, PhD) typically lead to higher salaries, though experience and tool knowledge are also highly valuable.

6–9. Tools and Techniques:
 - ML Algorithms: Gradient Boosting Machines (e.g., XGBoost, LightGBM) (0.0179)
 - ML Frameworks: LightGBM (0.0151)
 - Transformer Models (e.g., BERT, GPT) (0.0150)
 - Online Learning Platforms: Coursera, EdX (0.0132)
 Familiarity with these technologies reflects industry relevance and practical skill sets, which employers are willing to pay for.

10–14. Additional Technical & Learning Indicators:
 - Programming in C++ and Go (0.0116, 0.0096): Indicates more advanced or performance-oriented skillsets.
 - Kaggle Participation (0.0113): Active community engagement and portfolio-building via Kaggle is a strong positive signal.
 - Course Platforms (Coursera) (0.0102): Students who use high-quality learning platforms may gain practical skills that are rewarded in the job market.
 - Gender (0.0109): Gender appeared as a statistically significant feature, but interpretation requires caution due to complex socioeconomic and structural factors.

Summary:

Salaries in data science are shaped by a combination of where you live, how much experience you have, what tools and techniques you’ve mastered, and how you’ve invested in learning. The most impactful features suggest that a successful—and well-paid—career in data science depends not just on formal education, but also on skill-specific development, real-world tool use, and active engagement with the data science community.
"""

import pandas as pd
from ast import literal_eval
from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load CSV file
file_path = '/content/drive/MyDrive/BUS 458/Copy of kaggle_survey_2022_cleaned-5.csv'
df = pd.read_csv('/content/drive/MyDrive/BUS 458 /Copy of kaggle_survey_2022_cleaned-5.csv')

# STEP 1: Rename columns for clarity (optional but helpful)
df = df.rename(columns={
    "What is your age?": "Age",
    "What is your gender?": "Gender",
    "In which country do you currently reside?": "Country",
    "On which platforms have you begun or completed data science courses?": "DS_Courses_Platforms",
    "What products or platforms did you find to be most helpful when you first started?": "Helpful_Products",
    "What is the highest level of formal education that you have attained or plan to attain within the next 2 years?": "Education_Level",
    "For how many years have you been writing code and/or programming?": "Years_Coding",
    "What programming languages do you use on a regular basis?": "Languages",
    "Which machine learning frameworks do you use on a regular basis?": "ML_Frameworks",
    "Which ML algorithms do you use on a regular basis?": "ML_Algorithms",
    "What is your annual compensation (USD)?": "Salary",  # Renamed for consistency
    "How much money have you spent on ML/Cloud services in the last 5 years?": "ML_Cloud_Spend",
    "Who/what are your favorite media sources that report on data science topics?": "Media_Sources"
})

# STEP 2: Parse salary column
def parse_salary(s):
    if pd.isna(s): return None  # Handle missing values appropriately
    if '-' in s:
        s = s.replace(',', '').replace('$', '')
        low, high = s.split('-')
        return (int(low) + int(high)) / 2
    elif 'less' in s.lower():
        return 5000  # Assuming a value for "less than..."
    elif 'more' in s.lower():
        return 500000  # Assuming a value for "more than..."
    else:
        return pd.to_numeric(s.replace(',', '').replace('$', ''), errors='coerce')

df['Salary_Cleaned'] = df['Salary'].apply(parse_salary)

# STEP 3: Encode categorical single-select features
cat_cols = ['Age', 'Gender', 'Country', 'Education_Level', 'Years_Coding', 'ML_Cloud_Spend']
for col in cat_cols:
    df[col] = df[col].fillna('Missing')
    df[col] = LabelEncoder().fit_transform(df[col])

# STEP 4: Process multi-select columns using MultiLabelBinarizer
multi_cols = ['Languages', 'ML_Frameworks', 'ML_Algorithms', 'DS_Courses_Platforms', 'Helpful_Products', 'Media_Sources']
for col in multi_cols:
    df[col] = df[col].apply(lambda x: literal_eval(x) if pd.notnull(x) else [])

mlb = MultiLabelBinarizer()
all_multi = pd.DataFrame()

for col in multi_cols:
    transformed = mlb.fit_transform(df[col])
    col_names = [f"{col[:20]}_{x}" for x in mlb.classes_]
    temp_df = pd.DataFrame(transformed, columns=col_names)
    all_multi = pd.concat([all_multi, temp_df], axis=1)

# STEP 5: Prepare features and target
features = df[cat_cols].join(all_multi).fillna(0)
target = df['Salary_Cleaned']

# Remove rows with missing salary values (NaNs)
df = df.dropna(subset=['Salary_Cleaned'])  # Drop NaNs here

# Proceed with modeling
features = df[cat_cols].join(all_multi).fillna(0)
target = df['Salary_Cleaned']

# STEP 6: Train/Test Split and Random Forest Model
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# STEP 7: Feature Importance
importances = pd.Series(model.feature_importances_, index=features.columns)
top_features = importances.sort_values(ascending=False).head(15)

# Display top features
print("Top 15 Most Important Features Predicting Salary:")
print(top_features)

# Optional: Plot
top_features.sort_values().plot(kind='barh', figsize=(9, 6), title="Top 15 Salary Predictors")
plt.xlabel("Importance Score")
plt.show()

"""2. What are the most prevalent tools (software) and techniques (methods) being applied by Data Scientists today?

**Answer:**To identify the most widely used tools and techniques in the analytics profession, we analyzed survey responses from thousands of data scientists. The top responses indicate a strong preference for open-source programming languages, widely adopted machine learning frameworks, and classic statistical methods.

Most Prevalent Tools and Techniques (based on frequency of selection):

1. Python (6,521 respondents)
 Python remains the dominant programming language in data science due to its versatility, large community, and vast ecosystem of libraries for data manipulation, modeling, and visualization.

2. Scikit-learn (4,737)
 A popular Python library for classical machine learning methods, Scikit-learn is widely used for tasks like classification, regression, and clustering.

3. Linear or Logistic Regression (4,663)
 Despite the rise of advanced machine learning, foundational statistical models like linear and logistic regression continue to be heavily used for their interpretability and effectiveness in many business contexts.

4. Decision Trees and Random Forests (3,982)
 Tree-based models are favored for their accuracy, ease of use, and ability to handle both numeric and categorical data.

5. SQL (3,859)
 Data manipulation and extraction using SQL remains a core skill in the data science workflow, particularly when working with large structured datasets stored in relational databases.

6. TensorFlow (3,070) and Keras (2,609)
 These deep learning frameworks are popular for building neural networks and are especially common in areas like computer vision, natural language processing, and AI research.

7. Gradient Boosting Machines (e.g., XGBoost, LightGBM) (2,730)
 These ensemble methods are widely used in high-stakes modeling competitions (e.g., Kaggle) and production-grade systems due to their predictive power and scalability.

8. Convolutional Neural Networks (CNNs) (2,433)
 CNNs are essential for image-related tasks, highlighting the growing application of deep learning in areas like computer vision and autonomous systems.

9. XGBoost (2,219)
 A specific implementation of gradient boosting that is particularly well-known for its performance and speed, often used in winning solutions in predictive modeling competitions.

Key Insight:

Today’s data scientists commonly work with a mix of classic statistical techniques (like regression), ensemble tree models, and advanced deep learning tools. Python is the central hub connecting these tools, and libraries like Scikit-learn, TensorFlow, and XGBoost are standard components in the modern data science toolkit. Meanwhile, SQL continues to be a must-have for accessing and querying data in real-world environments.     
"""

import pandas as pd
from ast import literal_eval

# Mount Google Drive if you haven't already
from google.colab import drive
drive.mount('/content/drive')

# Load the dataset
file_path = '/content/drive/MyDrive/BUS 458/Copy of kaggle_survey_2022_cleaned-5.csv'
df = pd.read_csv('/content/drive/MyDrive/BUS 458 /Copy of kaggle_survey_2022_cleaned-5.csv')

# Convert the 3 relevant columns from stringified lists to actual lists
cols_to_parse = [
    'What programming languages do you use on a regular basis?',
    'Which machine learning frameworks do you use on a regular basis?',
    'Which ML algorithms do you use on a regular basis?'
]

for col in cols_to_parse:
    df[col] = df[col].apply(literal_eval)

# Combine all lists into a single column
combined_tools = df[cols_to_parse[0]] + df[cols_to_parse[1]] + df[cols_to_parse[2]]

# Flatten the list and count frequencies
all_tools_flat = combined_tools.explode()
tool_counts = Counter(all_tools_flat)

# Show top 10 tools/techniques
top_tools = pd.Series(tool_counts).sort_values(ascending=False).head(11)
print(top_tools)

import matplotlib.pyplot as plt

# Plot the top tools/techniques
plt.figure(figsize=(10, 6))
top_tools.plot(kind='barh', color='skyblue')
plt.xlabel('Usage Count')
plt.title('Top Tools and Techniques Used by Data Scientists')
plt.gca().invert_yaxis()  # Highest count at the top
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

"""3. What tools and techniques are emerging in the field of Data Science?
**Answer:**
🧠 Emerging Tools & Techniques in Data Science

We identified several tools with relatively low current usage that show strong signs of growth and relevance — marking them as “emerging” in the field. Then we went even further to research which of these tools/techniques are new in the or have gained popularility in the past 10 years:

1. Hugging Face – A fast-growing NLP library offering cutting-edge transformer models like BERT and GPT, simplifying complex language tasks.

2. Graph Neural Networks – Designed for graph-structured data (e.g., social networks, molecular data), gaining momentum in modern ML problems.

3. Recurrent Neural Networks – While older in theory, RNNs (especially LSTMs/GRUs) rose to real-world prominence in the last decade, powering time series, speech, and sequence modeling.

4. Go – A newer systems language increasingly used in data infrastructure and scalable ML backends, valued for speed and simplicity.

📊 Why These Stand Out

Lower usage ≠ less important — it often signals emerging potential. These tools are:

Technically advanced
Gaining adoption in research and industry
Strategic areas for students to upskill
They represent where data science is going, not just where it’s been. Want me to turn this into slide-ready bullets or a visual summary?
"""

import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load dataset
file_path = '/content/drive/MyDrive/BUS 458/Copy of kaggle_survey_2022_cleaned-5.csv'
df = pd.read_csv('/content/drive/MyDrive/BUS 458 /Copy of kaggle_survey_2022_cleaned-5.csv')

# Define columns related to tools and techniques
tool_cols = [
    "What programming languages do you use on a regular basis?",
    "Which machine learning frameworks do you use on a regular basis?",
    "Which ML algorithms do you use on a regular basis?"
]

# Count frequencies of each tool/technique
tool_counter = Counter()
for col in tool_cols:
    df[col] = df[col].fillna("")
    for row in df[col]:
        for item in row.split(','):
            cleaned = item.strip()
            if cleaned and cleaned != "No Selection":
                tool_counter[cleaned] += 1

# Convert to DataFrame and sort ascending to find emerging tools
emerging_df = pd.DataFrame.from_dict(tool_counter, orient='index', columns=['count'])
emerging_df = emerging_df.sort_values('count', ascending=True)

# Filter: show only tools used at least 10 times (remove noise)
emerging_df = emerging_df[emerging_df['count'] >= 10]

# Take top 15 least-used (i.e. emerging) tools/techniques
top_emerging = emerging_df.head(15)

# Display the least-used tools = potentially emerging
print("🔍 Emerging Tools and Techniques (Used by Few Respondents):")
print(emerging_df.head(15))

import matplotlib.pyplot as plt

# Emerging tools and their approximate usage counts
emerging_tools = {
    'Hugging Face': 13,
    'Graph Neural Networks': 25,
    'Recurrent Neural Networks': 12,
    'Go': 19
}

# Sort tools by usage for cleaner visualization
tools_sorted = dict(sorted(emerging_tools.items(), key=lambda x: x[1], reverse=True))

# Plotting
plt.figure(figsize=(8, 5))
plt.barh(list(tools_sorted.keys()), list(tools_sorted.values()), color='teal')
plt.xlabel('Usage Count')
plt.title('Emerging Tools & Techniques in Data Science')
plt.gca().invert_yaxis()  # Highest value on top
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""4. How and where should aspiring data scientist invest their time and energy to prepare for the current and future Data Science environment?

**Answer**: Aspiring data scientists should prioritize learning foundational tools like Python, SQL, and Scikit-learn, as these are the most commonly used in the field. They should also invest time in mastering key techniques such as regression, decision trees, and neural networks. For education, online platforms like Coursera, Kaggle, and YouTube offer flexible and effective learning paths. Starting with these platforms can provide both theoretical knowledge and practical experience. To stay future-ready, students should also explore emerging tools like TensorFlow, Keras, and advanced methods like Gradient Boosting and Transformers.
"""

import pandas as pd
from ast import literal_eval
from collections import Counter

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load dataset
file_path = '/content/drive/MyDrive/BUS 458/Copy of kaggle_survey_2022_cleaned-5.csv'
df = pd.read_csv('/content/drive/MyDrive/BUS 458 /Copy of kaggle_survey_2022_cleaned-5.csv')

# Columns to parse
cols_to_parse = [
    "On which platforms have you begun or completed data science courses?",
    "What products or platforms did you find to be most helpful when you first started?",
    "What programming languages do you use on a regular basis?",
    "Which machine learning frameworks do you use on a regular basis?",
    "Which ML algorithms do you use on a regular basis?"
]

# Convert stringified lists to actual Python lists
for col in cols_to_parse:
    df[col] = df[col].apply(lambda x: literal_eval(x) if pd.notnull(x) else [])

# Define the count_top_items function
def count_top_items(series, n=10):
  """Counts the top n most frequent items in a Pandas Series.

  Args:
      series: A Pandas Series containing the items to count.
      n: The number of top items to return.

  Returns:
      A Pandas Series with the top n items and their counts.
  """
  all_items = series.explode()
  return all_items.value_counts().head(n)

# Count most helpful platforms
helpful = df["What products or platforms did you find to be most helpful when you first started?"].explode()
helpful_counts = helpful.value_counts().head(10)

# Count most used course platforms
courses = df["On which platforms have you begun or completed data science courses?"].explode()
course_counts = courses.value_counts().head(10)

# Combine all tool-related lists and count usage
skills = (
    df["What programming languages do you use on a regular basis?"] +
    df["Which machine learning frameworks do you use on a regular basis?"] +
    df["Which ML algorithms do you use on a regular basis?"]
).explode()
skill_counts = skills.value_counts().head(10)

# Print results
print("🏆 Top Helpful Platforms for Beginners:")
print(helpful_counts)

print("\n📘 Top Data Science Course Platforms:")
print(course_counts)

print("\n🛠️ Most Commonly Used Tools & Techniques:")
print(skill_counts)

# Set plot style
sns.set(style="whitegrid")
plt.figure(figsize=(16, 12))

# Plot 1: Helpful platforms
plt.subplot(3, 1, 1)
sns.barplot(x=top_helpful.values, y=top_helpful.index, palette="crest")
plt.title("Top Helpful Platforms for Beginners")
plt.xlabel("Mentions")
plt.ylabel("Platform")

# Plot 2: Course platforms
plt.subplot(3, 1, 2)
sns.barplot(x=top_courses.values, y=top_courses.index, palette="mako")
plt.title("Top Data Science Course Platforms")
plt.xlabel("Mentions")
plt.ylabel("Platform")

# Plot 3: Common tools & techniques
plt.subplot(3, 1, 3)
sns.barplot(x=top_skills.values, y=top_skills.index, palette="flare")
plt.title("Most Commonly Used Tools & Techniques")
plt.xlabel("Mentions")
plt.ylabel("Tool or Technique")

plt.tight_layout()
plt.show()

"""5. Is formal education important to success as a Data Scientist?

**Answer:** Based on the average salary data by education level, formal education appears to be an important factor in achieving higher earnings as a Data Scientist:

Those with no formal education or incomplete college earn around $36,000–$45,000.
Bachelor's and Master's degree holders earn significantly more, averaging around $40,000–$53,000.
The highest salaries are seen among those with Doctoral or Professional Doctorate degrees, reaching $52,000–$66,000 on average.
This suggests a clear trend: higher levels of formal education are correlated with higher salaries in the field of data science. While it's still possible to succeed without a degree, investing in formal education—especially at the graduate level—can significantly enhance career prospects and earning potential.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load dataset
file_path = '/content/drive/MyDrive/BUS 458/Copy of kaggle_survey_2022_cleaned-5.csv'
df = pd.read_csv('/content/drive/MyDrive/BUS 458 /Copy of kaggle_survey_2022_cleaned-5.csv')

# Rename for ease
df = df.rename(columns={
    "What is the highest level of formal education that you have attained or plan to attain within the next 2 years?": "Education_Level",
    "Salary": "Salary"
})

# Drop missing values
df = df[["Education_Level", "Salary"]].dropna()

# If Salary is a string like "$0-999", convert to midpoint (if not, skip this)
salary_map = {
    '$0-999': 500,
    '1,000-1,999': 1500,
    '2,000-2,999': 2500,
    '3,000-3,999': 3500,
    '4,000-4,999': 4500,
    '5,000-7,499': 6250,
    '7,500-9,999': 8750,
    '10,000-14,999': 12500,
    '15,000-19,999': 17500,
    '20,000-24,999': 22500,
    '25,000-29,999': 27500,
    '30,000-39,999': 35000,
    '40,000-49,999': 45000,
    '50,000-59,999': 55000,
    '60,000-69,999': 65000,
    '70,000-79,999': 75000,
    '80,000-89,999': 85000,
    '90,000-99,999': 95000,
    '100,000-124,999': 112500,
    '125,000-149,999': 137500,
    '150,000-199,999': 175000,
    '200,000-249,999': 225000,
    '250,000-299,999': 275000,
    '300,000-499,999': 400000,
    '500,000-999,999': 750000,
    '1,000,000+': 1000000
}
df["Salary"] = df["Salary"].map(salary_map)
df = df.dropna()

# Group and calculate average salary
avg_salary = df.groupby("Education_Level")["Salary"].mean().sort_values() # Removed the underscore (_) from sort_values_

# Print results
print("📊 Average Salary by Education Level:\n")
print(avg_salary.round(0).astype(int))

# Plot
plt.figure(figsize=(10, 6))
sns.barplot(x=avg_salary.values, y=avg_salary.index, palette="mako")
plt.title("Average Salary by Education Level", fontsize=16)
plt.xlabel("Average Salary (USD)")
plt.ylabel("Education Level")
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

"""6.	How does the return on formal education compare to other types of learning?

**Answer:** Based on the results, formal education—represented by university courses—yields the highest average salary ($71,215.38), indicating the strongest return among all learning paths. However, alternative methods such as "Other" resources ($68,188.73) and social media platforms ($65,903.36) also provide strong returns, suggesting that self-directed and community-based learning can be nearly as valuable.

Meanwhile, more accessible platforms like online courses ($56,916.83), Kaggle ($54,332.12), and video platforms like YouTube ($52,982.21) offer respectable returns, though slightly lower. These methods are likely more flexible and affordable, making them attractive for many aspiring data scientists.

In summary:  
🧑‍🎓 Formal education leads to the highest average salary.  
💻 Non-traditional learning (social media, online courses, Kaggle) shows strong potential and offers high ROI for those without formal degrees.  
📚 A hybrid approach—combining formal education with practical platforms—might offer the best overall preparation for a data science career.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from ast import literal_eval

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load dataset
file_path = '/content/drive/MyDrive/BUS 458/Copy of kaggle_survey_2022_cleaned-5.csv'
df = pd.read_csv('/content/drive/MyDrive/BUS 458 /Copy of kaggle_survey_2022_cleaned-5.csv')

# Convert stringified list to actual list
df['What products or platforms did you find to be most helpful when you first started?'] = df['What products or platforms did you find to be most helpful when you first started?'].apply(
    lambda x: literal_eval(x) if pd.notnull(x) else [])

# Explode the multi-select list into separate rows
exploded = df.explode('What products or platforms did you find to be most helpful when you first started?')

# --- Convert 'Salary' column to numeric before calculating mean ---
def parse_salary(salary_str):
    if isinstance(salary_str, str):  # Check if the value is a string
        salary_str = salary_str.replace('$', '').replace(',', '')
        if '-' in salary_str:
            low, high = salary_str.split('-')
            return (float(low) + float(high)) / 2
        elif salary_str.startswith('>'):
            return float(salary_str[1:])  # Handle ">1,000,000" case
        else:
            try:
                return float(salary_str)
            except ValueError:
                return float('nan')  # Handle non-numeric or invalid cases
    else:
        return salary_str

exploded['Salary'] = exploded['Salary'].apply(parse_salary)
exploded = exploded.dropna(subset=['Salary']) # Remove rows where Salary could not be parsed to float

# Group by platform/product and calculate average salary
avg_salary_by_platform = exploded.groupby('What products or platforms did you find to be most helpful when you first started?')['Salary'].mean().sort_values(ascending=False).dropna()

# Display the results as text
print("💰 Average Salary by Most Helpful Platform:\n")
print(avg_salary_by_platform.round(2))

# Plotting
plt.figure(figsize=(12, 6))
sns.barplot(x=avg_salary_by_platform.values, y=avg_salary_by_platform.index, palette='viridis')
plt.xlabel('Average Salary')
plt.ylabel('Product or Platform')
plt.title('Average Salary by Most Helpful Learning Platform')
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""**Answer**:

7.	How should educational institutions think about the role of formal education in the world of data science? Are there any specific recommendations in terms of methodologies that institutions of formal education should employ in the training of data scientists?

**Answer**: Educational institutions should recognize that while formal education remains valuable, it is no longer the sole path to success in the data science field. Our analysis shows that those with doctoral and master’s degrees tend to earn the highest average salaries, indicating that formal education can offer a strong return on investment. However, individuals who found platforms like Kaggle, YouTube, and Coursera most helpful also earn competitive salaries—suggesting that practical, hands-on learning is essential and highly valued in the industry.

Therefore, institutions should blend traditional instruction with applied learning experiences. Recommendations include:

Integrating online resources (like Coursera, Kaggle, and YouTube) into coursework to supplement theory with practice.
Focusing on project-based learning that mirrors real-world data science problems.
Encouraging the use of collaborative platforms (e.g., GitHub, Kaggle) to build portfolios and gain feedback from the data science community.
Teaching current tools and techniques (e.g., Python, Scikit-learn, XGBoost) and staying updated with emerging trends (e.g., Huggingface, Graph Neural Networks).
By combining academic rigor with practical skill-building, institutions can better prepare students for both current and future demands in the data science profession.

8. In addition, you need to create a tool (only a model…does not need to be a production piece of software) that students and young professionals can use to input their information and get a predicted income (you also need to validate that the model works). Make sure to include the model results and performance in your presentation. - gradio !

#Getting Data Ready for Gradio

Based on the randomn tree model our team made, we need to edit some of the columns to be able to successful create the Gradio app. These questions are:

1. What is your age?
2. What country do you reside in?

**First install Gradio**
"""

!pip install gradio --quiet

"""**Question 1: What is your age?**

 Since the age column is categorical (ordinal) in your case — with age binned into ranges — you can use a dropdown menu in Gradio so users select their age group easily.
"""

import gradio as gr

# Define the age group choices
age_groups = [
    "18-21", "22-24", "25-29", "30-34", "35-39",
    "40-44", "45-49", "50-54", "55-59", "60-69", "70+"
]

# Example function to handle the selected age group
def handle_age_selection(age_group):
    return f"You selected the age group: {age_group}"

# Gradio interface
demo = gr.Interface(
    fn=handle_age_selection,
    inputs=gr.Dropdown(choices=age_groups, label="Select your age group"),
    outputs="text"
)

# ✅ Launch the app with sharing and debug enabled for Colab
demo.launch(share=True, debug=True)

"""**Question 2: What country do you reside in?**

**Reasoning**: To prevent overfitting (a model might wrongly assume "Country X = Low salary". based on only 2 responses for example, or even just to simplify the Gradio App, that indefintely overwhelm the user to how a drop down of 150+ countries, there must be an option to other "Other' for the country question.
"""